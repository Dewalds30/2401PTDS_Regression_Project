{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18d26de0-c9a3-4672-a5f2-6fbc5594d403",
   "metadata": {},
   "source": [
    "# 1. Imports & Global config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bce9836-f5c4-4146-828c-ce6c1a73a6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_validate, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_squared_log_error, make_scorer\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "import joblib\n",
    "\n",
    "# Reusable settings\n",
    "RANDOM_STATE = 42\n",
    "N_SPLITS = 5\n",
    "TARGET = \"total_emission\"\n",
    "BASE_DIR = Path(r\"C:\\Users\\Dewald\\Documents\\Regression Project\")  # <-- your path\n",
    "xlsx_path = BASE_DIR / \"co2_emissions_from_agri_cleaned.xlsx\"\n",
    "csv_path  = BASE_DIR / \"co2_emissions_from_agri_cleaned.csv\"\n",
    "\n",
    "def rmsle_safe(y_true, y_pred, eps=1e-9):\n",
    "    y_true = np.clip(np.asarray(y_true), a_min=eps, a_max=None)\n",
    "    y_pred = np.clip(np.asarray(y_pred), a_min=eps, a_max=None)\n",
    "    try:\n",
    "        return math.sqrt(mean_squared_log_error(y_true, y_pred))\n",
    "    except ValueError:\n",
    "        return np.nan\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return float(np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "\n",
    "def cv_scorers():\n",
    "    # negative RMSE so GridSearchCV can \"maximize\"\n",
    "    return {\n",
    "        \"rmse\": make_scorer(lambda yt, yp: rmse(yt, yp), greater_is_better=False),\n",
    "        \"r2\": \"r2\"\n",
    "    }\n",
    "\n",
    "def print_test_report(name, y_true, y_pred):\n",
    "    print(f\"\\n{name} — Test set metrics\")\n",
    "    print(f\"  RMSE : {rmse(y_true, y_pred):.6f}\")\n",
    "    print(f\"  R²   : {r2_score(y_true, y_pred):.6f}\")\n",
    "    print(f\"  RMSLE: {rmsle_safe(y_true, y_pred):.6f}\")\n",
    "\n",
    "def extract_importances_or_coefs(pipe, feature_names):\n",
    "    \"\"\"Return DataFrame with ['feature','importance'] if available; else None.\"\"\"\n",
    "    reg = pipe[-1] if not hasattr(pipe, \"named_steps\") else pipe.named_steps.get(\"reg\", None)\n",
    "    if reg is None:\n",
    "        return None\n",
    "    # LinearRegression => absolute coefficients (on scaled features)\n",
    "    if isinstance(reg, LinearRegression):\n",
    "        coef = getattr(reg, \"coef_\", None)\n",
    "        if coef is None:\n",
    "            return None\n",
    "        coefs = np.abs(np.ravel(coef))\n",
    "        return pd.DataFrame({\"feature\": feature_names, \"importance\": coefs}).sort_values(\"importance\", ascending=False)\n",
    "    # Tree-based => feature_importances_\n",
    "    fi = getattr(reg, \"feature_importances_\", None)\n",
    "    if fi is None:\n",
    "        return None\n",
    "    return pd.DataFrame({\"feature\": feature_names, \"importance\": fi}).sort_values(\"importance\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01821043-11ff-4102-937f-24e6e1c2dc50",
   "metadata": {},
   "source": [
    "# 2. Load & basic cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80cad0a7-1106-4fa0-a9f0-3b864b3be272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Excel: C:\\Users\\Dewald\\Documents\\Regression Project\\co2_emissions_from_agri_cleaned.xlsx\n",
      "Data ready. Rows: 6,965; Numeric features used: 28\n"
     ]
    }
   ],
   "source": [
    "if xlsx_path.exists():\n",
    "    print(f\"Loading Excel: {xlsx_path}\")\n",
    "    df = pd.read_excel(xlsx_path, sheet_name=0)\n",
    "elif csv_path.exists():\n",
    "    print(f\"Loading CSV:   {csv_path}\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "else:\n",
    "    raise FileNotFoundError(\n",
    "        f\"Could not find data file in:\\n - {xlsx_path}\\n - {csv_path}\\n\"\n",
    "        \"Place your cleaned dataset there with one of these names.\"\n",
    "    )\n",
    "\n",
    "# Drop unnamed helper columns\n",
    "for col in list(df.columns):\n",
    "    if str(col).lower().startswith(\"unnamed\"):\n",
    "        df.drop(columns=[col], inplace=True)\n",
    "\n",
    "assert TARGET in df.columns, f\"Expected target column '{TARGET}' in columns.\"\n",
    "\n",
    "# Keep numeric features (excluding target)\n",
    "numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c]) and c != TARGET]\n",
    "if not numeric_cols:\n",
    "    raise ValueError(\"No numeric feature columns (besides target) were found.\")\n",
    "\n",
    "X = df[numeric_cols].copy()\n",
    "y = df[TARGET].copy()\n",
    "\n",
    "# Clean NaNs\n",
    "mask = y.notna()\n",
    "X, y = X.loc[mask], y.loc[mask]\n",
    "X = X.dropna(axis=0)\n",
    "y = y.loc[X.index]\n",
    "\n",
    "print(f\"Data ready. Rows: {len(df):,}; Numeric features used: {len(numeric_cols)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed9abbb-947c-4a76-a74d-0a2abfceed10",
   "metadata": {},
   "source": [
    "# 3. Train / Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a95f2e02-9bcd-4597-b941-05bd23a3beb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 5,572 | Test size: 1,393\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=RANDOM_STATE\n",
    ")\n",
    "feature_names = list(X_train.columns)\n",
    "\n",
    "print(f\"Train size: {X_train.shape[0]:,} | Test size: {X_test.shape[0]:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67a4203-406b-40d3-8b64-49be0faf374e",
   "metadata": {},
   "source": [
    "# 4. Linear Regression + CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d82f74e1-31d0-4f3f-9dba-27702d665188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression — CV results\n",
      "  RMSE (mean±sd): 1800885.499945 ± 281991.562235\n",
      "  R²   (mean±sd): 0.997797 ± 0.000562\n",
      "\n",
      "LinearRegression — Test set metrics\n",
      "  RMSE : 1978461.775516\n",
      "  R²   : 0.998382\n",
      "  RMSLE: 7.424983\n",
      "Saved:\n",
      "  C:\\Users\\Dewald\\Documents\\Regression Project\\linear_regression_predictions_test.csv\n",
      "  C:\\Users\\Dewald\\Documents\\Regression Project\\model_linear_regression.joblib\n"
     ]
    }
   ],
   "source": [
    "linr_pipe = Pipeline([\n",
    "    (\"prep\", ColumnTransformer([(\"num\", StandardScaler(), feature_names)], remainder=\"drop\")),\n",
    "    (\"reg\", LinearRegression())\n",
    "])\n",
    "\n",
    "cv = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "scores_linr = cross_validate(\n",
    "    linr_pipe, X_train, y_train,\n",
    "    scoring=cv_scorers(), cv=cv, return_train_score=False\n",
    ")\n",
    "print(\"LinearRegression — CV results\")\n",
    "print(f\"  RMSE (mean±sd): {(-scores_linr['test_rmse']).mean():.6f} ± {(-scores_linr['test_rmse']).std():.6f}\")\n",
    "print(f\"  R²   (mean±sd): { (scores_linr['test_r2']).mean():.6f} ± { (scores_linr['test_r2']).std():.6f}\")\n",
    "\n",
    "linr_pipe.fit(X_train, y_train)\n",
    "linr_pred = linr_pipe.predict(X_test)\n",
    "print_test_report(\"LinearRegression\", y_test, linr_pred)\n",
    "\n",
    "# Save artifacts for this model\n",
    "OUT_DIR = BASE_DIR\n",
    "linr_pred_path = OUT_DIR / \"linear_regression_predictions_test.csv\"\n",
    "pd.DataFrame({\"y_true\": y_test.values, \"y_pred\": linr_pred, \"residual\": y_test.values - linr_pred},\n",
    "             index=y_test.index).to_csv(linr_pred_path)\n",
    "\n",
    "linr_model_path = OUT_DIR / \"model_linear_regression.joblib\"\n",
    "joblib.dump(linr_pipe, linr_model_path, compress=3)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" \", linr_pred_path)\n",
    "print(\" \", linr_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3babf247-055b-4e25-bba1-c0b2ebe2baff",
   "metadata": {},
   "source": [
    "# 4a. Random Forest + GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f58be124-fda3-4df1-af83-c0a2dbdb6bb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "\n",
      "RandomForest — best CV params: {'reg__max_depth': None, 'reg__min_samples_leaf': 1, 'reg__n_estimators': 400}\n",
      "\n",
      "RandomForest (best grid) — Test set metrics\n",
      "  RMSE : 235861.154027\n",
      "  R²   : 0.999977\n",
      "  RMSLE: 0.456070\n",
      "Saved:\n",
      "  C:\\Users\\Dewald\\Documents\\Regression Project\\random_forest_predictions_test.csv\n",
      "  C:\\Users\\Dewald\\Documents\\Regression Project\\model_random_forest.joblib\n"
     ]
    }
   ],
   "source": [
    "# === D2) Random Forest + GridSearchCV ===\n",
    "rf_pipe = Pipeline([\n",
    "    (\"prep\", \"passthrough\"),  # trees don't need scaling\n",
    "    (\"reg\", RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=-1))\n",
    "])\n",
    "\n",
    "rf_param_grid = {\n",
    "    \"reg__n_estimators\": [200, 400],\n",
    "    \"reg__max_depth\": [None, 10, 20],\n",
    "    \"reg__min_samples_leaf\": [1, 2, 4]\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "rf_grid = GridSearchCV(\n",
    "    rf_pipe, rf_param_grid, cv=cv,\n",
    "    scoring=cv_scorers(), refit=\"rmse\",  # refit on negative-RMSE scorer\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "rf_grid.fit(X_train, y_train)\n",
    "print(\"\\nRandomForest — best CV params:\", rf_grid.best_params_)\n",
    "\n",
    "rf_best = rf_grid.best_estimator_\n",
    "rf_pred = rf_best.predict(X_test)\n",
    "print_test_report(\"RandomForest (best grid)\", y_test, rf_pred)\n",
    "\n",
    "# Save artifacts for this model\n",
    "OUT_DIR = BASE_DIR\n",
    "rf_pred_path = OUT_DIR / \"random_forest_predictions_test.csv\"\n",
    "pd.DataFrame({\"y_true\": y_test.values, \"y_pred\": rf_pred, \"residual\": y_test.values - rf_pred},\n",
    "             index=y_test.index).to_csv(rf_pred_path)\n",
    "\n",
    "rf_model_path = OUT_DIR / \"model_random_forest.joblib\"\n",
    "joblib.dump(rf_best, rf_model_path, compress=3)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" \", rf_pred_path)\n",
    "print(\" \", rf_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8b9f9f-bbc0-4344-ba93-f92466cc3035",
   "metadata": {},
   "source": [
    "# 4b. Gradient Boosting + GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8f37e62-f465-428c-8d3b-a0b529fe48f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
      "\n",
      "GradientBoosting — best CV params: {'reg__learning_rate': 0.1, 'reg__max_depth': 2, 'reg__min_samples_leaf': 1, 'reg__n_estimators': 400}\n",
      "\n",
      "GradientBoosting (best grid) — Test set metrics\n",
      "  RMSE : 245403.746863\n",
      "  R²   : 0.999975\n",
      "  RMSLE: 1.120372\n",
      "Saved:\n",
      "  C:\\Users\\Dewald\\Documents\\Regression Project\\gradient_boosting_predictions_test.csv\n",
      "  C:\\Users\\Dewald\\Documents\\Regression Project\\model_gradient_boosting.joblib\n"
     ]
    }
   ],
   "source": [
    "gb_pipe = Pipeline([\n",
    "    (\"prep\", \"passthrough\"),\n",
    "    (\"reg\", GradientBoostingRegressor(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "gb_param_grid = {\n",
    "    \"reg__n_estimators\": [200, 400],\n",
    "    \"reg__learning_rate\": [0.05, 0.1],\n",
    "    \"reg__max_depth\": [2, 3],\n",
    "    \"reg__min_samples_leaf\": [1, 3]\n",
    "}\n",
    "\n",
    "cv = KFold(n_splits=N_SPLITS, shuffle=True, random_state=RANDOM_STATE)\n",
    "gb_grid = GridSearchCV(\n",
    "    gb_pipe, gb_param_grid, cv=cv,\n",
    "    scoring=cv_scorers(), refit=\"rmse\",\n",
    "    n_jobs=-1, verbose=1\n",
    ")\n",
    "gb_grid.fit(X_train, y_train)\n",
    "print(\"\\nGradientBoosting — best CV params:\", gb_grid.best_params_)\n",
    "\n",
    "gb_best = gb_grid.best_estimator_\n",
    "gb_pred = gb_best.predict(X_test)\n",
    "print_test_report(\"GradientBoosting (best grid)\", y_test, gb_pred)\n",
    "\n",
    "# Save artifacts for this model\n",
    "OUT_DIR = BASE_DIR\n",
    "gb_pred_path = OUT_DIR / \"gradient_boosting_predictions_test.csv\"\n",
    "pd.DataFrame({\"y_true\": y_test.values, \"y_pred\": gb_pred, \"residual\": y_test.values - gb_pred},\n",
    "             index=y_test.index).to_csv(gb_pred_path)\n",
    "\n",
    "gb_model_path = OUT_DIR / \"model_gradient_boosting.joblib\"\n",
    "joblib.dump(gb_best, gb_model_path, compress=3)\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" \", gb_pred_path)\n",
    "print(\" \", gb_model_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa1b88a9-ab76-4ff6-b69e-9436bfd784f6",
   "metadata": {},
   "source": [
    "# 5. Compare models (test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9b45936-6783-4b91-b816-ad8433a513e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Model leaderboard (by Test RMSE) ===\n",
      "RandomForest        RMSE=235861.154027 | R²=0.999977 | RMSLE=0.456070\n",
      "GradientBoosting    RMSE=245403.746863 | R²=0.999975 | RMSLE=1.120372\n",
      "LinearRegression    RMSE=1978461.775516 | R²=0.998382 | RMSLE=7.424983\n"
     ]
    }
   ],
   "source": [
    "# This expects the variables linr_pred / rf_pred / gb_pred to exist depending on what you've run.\n",
    "results = []\n",
    "\n",
    "if 'linr_pred' in globals():\n",
    "    results.append((\"LinearRegression\", rmse(y_test, linr_pred), r2_score(y_test, linr_pred), rmsle_safe(y_test, linr_pred)))\n",
    "if 'rf_pred' in globals():\n",
    "    results.append((\"RandomForest\", rmse(y_test, rf_pred), r2_score(y_test, rf_pred), rmsle_safe(y_test, rf_pred)))\n",
    "if 'gb_pred' in globals():\n",
    "    results.append((\"GradientBoosting\", rmse(y_test, gb_pred), r2_score(y_test, gb_pred), rmsle_safe(y_test, gb_pred)))\n",
    "\n",
    "if not results:\n",
    "    raise RuntimeError(\"No models have been run yet. Execute one or more of D1-D3 first.\")\n",
    "\n",
    "results_sorted = sorted(results, key=lambda r: r[1])  # by RMSE asc\n",
    "print(\"\\n=== Model leaderboard (by Test RMSE) ===\")\n",
    "for name, rmse_v, r2_v, rmsle_v in results_sorted:\n",
    "    print(f\"{name:<18}  RMSE={rmse_v:.6f} | R²={r2_v:.6f} | RMSLE={rmsle_v:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4940dc54-64ef-4d8b-8aec-25eef99b4191",
   "metadata": {},
   "source": [
    "# 6. Save the single best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c540d0d3-bd78-4d65-a8d0-7950cfc9032a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model so far: RandomForest (RMSE=235861.154027, R²=0.999977, RMSLE=0.456070)\n",
      "Saved best model to: C:\\Users\\Dewald\\Documents\\Regression Project\\best_model_randomforest.joblib\n",
      "Saved metrics to: C:\\Users\\Dewald\\Documents\\Regression Project\\model_metrics_cv_test_session.csv\n"
     ]
    }
   ],
   "source": [
    "# === F) Save the single best model (of the ones you've run) ===\n",
    "# Recompute 'results_with_models' using whichever models you've already run:\n",
    "results_with_models = []\n",
    "\n",
    "if 'linr_pipe' in globals() and 'linr_pred' in globals():\n",
    "    results_with_models.append((\"LinearRegression\", rmse(y_test, linr_pred), r2_score(y_test, linr_pred), rmsle_safe(y_test, linr_pred), linr_pipe))\n",
    "if 'rf_best' in globals() and 'rf_pred' in globals():\n",
    "    results_with_models.append((\"RandomForest\", rmse(y_test, rf_pred), r2_score(y_test, rf_pred), rmsle_safe(y_test, rf_pred), rf_best))\n",
    "if 'gb_best' in globals() and 'gb_pred' in globals():\n",
    "    results_with_models.append((\"GradientBoosting\", rmse(y_test, gb_pred), r2_score(y_test, gb_pred), rmsle_safe(y_test, gb_pred), gb_best))\n",
    "\n",
    "if not results_with_models:\n",
    "    raise RuntimeError(\"No trained models found. Run D1–D3 first.\")\n",
    "\n",
    "results_with_models.sort(key=lambda r: r[1])  # by RMSE\n",
    "best_name, best_rmse, best_r2, best_rmsle, best_model = results_with_models[0]\n",
    "print(f\"Best model so far: {best_name} (RMSE={best_rmse:.6f}, R²={best_r2:.6f}, RMSLE={best_rmsle:.6f})\")\n",
    "\n",
    "OUT_DIR = BASE_DIR\n",
    "best_model_path = OUT_DIR / f\"best_model_{best_name.lower()}.joblib\"\n",
    "joblib.dump(best_model, best_model_path, compress=3)\n",
    "print(\"Saved best model to:\", best_model_path)\n",
    "\n",
    "# Also save a unified metrics CSV of only the models you've run in this session\n",
    "metrics_df = pd.DataFrame([{\n",
    "    \"model\": name, \"rmse\": rmse_v, \"r2\": r2_v, \"rmsle\": rmsle_v\n",
    "} for name, rmse_v, r2_v, rmsle_v, _ in results_with_models])\n",
    "metrics_path = OUT_DIR / \"model_metrics_cv_test_session.csv\"\n",
    "metrics_df.to_csv(metrics_path, index=False)\n",
    "print(\"Saved metrics to:\", metrics_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e688de5-0460-464b-bb99-619b7ac6935c",
   "metadata": {},
   "source": [
    "# 7. Feature importances / coefficients (Top 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71c9d327-b237-4680-a6fd-6a611fa01d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved top 20 importances/coefficients to: C:\\Users\\Dewald\\Documents\\Regression Project\\randomforest_feature_importances_top20.csv\n"
     ]
    }
   ],
   "source": [
    "# === G) Feature importances / coefficients (Top 20) ===\n",
    "fi_df = extract_importances_or_coefs(best_model, feature_names)\n",
    "if fi_df is not None and not fi_df.empty:\n",
    "    top20 = fi_df.head(20).copy()\n",
    "    top20_path = BASE_DIR / f\"{best_name.lower()}_feature_importances_top20.csv\"\n",
    "    top20.to_csv(top20_path, index=False)\n",
    "    print(\"Saved top 20 importances/coefficients to:\", top20_path)\n",
    "else:\n",
    "    print(\"No feature importances/coefficients available for this model.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
